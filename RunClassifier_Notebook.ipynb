{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/descobarsalce/Classifier_BERT/blob/main/RunClassifier_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c07a7213",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c07a7213",
        "outputId": "1a325f62-5b0c-4704-fad5-43579a251fb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat 'C:Usersdescobar-salceOneDriveDocumentsTextClassification_Python/*': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "# prompt: transfer all data from local folder on my physical drive located in \"C:\\Users\\descobar-salce\\OneDrive - Eastern Research Group\\Documents\\TextClassification_Python\" to my google colab folder. This is not pandas, but several .py files in the folder\n",
        "\n",
        "!cp -r C:\\Users\\descobar-salce\\OneDrive\\Documents\\TextClassification_Python/* /content/\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "282FjAIUeAJY"
      },
      "id": "282FjAIUeAJY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "W9PYyF2reATl"
      },
      "id": "W9PYyF2reATl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /home\n",
        "!mkdir TextClassification\n",
        "%cd /home/TextClassification"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObfkPSaYa33F",
        "outputId": "d30a2192-3cdd-48cd-b100-de52a8cbccd3"
      },
      "id": "ObfkPSaYa33F",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/home\n",
            "mkdir: cannot create directory ‘TextClassification’: File exists\n",
            "/home/TextClassification\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eAaM26Qia36T"
      },
      "id": "eAaM26Qia36T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KuUfzCHSd-cc"
      },
      "id": "KuUfzCHSd-cc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "46xaYiWsd-fd"
      },
      "id": "46xaYiWsd-fd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "cJk7GAgea3_b"
      },
      "id": "cJk7GAgea3_b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "069f70a7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "069f70a7",
        "outputId": "8abf1208-1644-45b5-a0c3-3a48330cd9f3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-b2d6fe5469c4>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mClassifierBERT\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClassifierBERT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhtml_parser\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHTML_folder_parser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ClassifierBERT'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from ClassifierBERT import ClassifierBERT\n",
        "from html_parser import HTML_folder_parser\n",
        "import pickle, json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0aec957",
      "metadata": {
        "id": "e0aec957"
      },
      "outputs": [],
      "source": [
        "project_prefix = \"WHD-2022-0003\"\n",
        "folder_path = \"DATA/WHD-2022-0003_comments_TEMP\"  # Replace with the path comments' folder\n",
        "\n",
        "# Load processed labels from\n",
        "labeled_data_path = 'DATA/labeled_data_' + project_prefix\n",
        "labeled_data = pd.read_pickle(labeled_data_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d5c9139",
      "metadata": {
        "id": "7d5c9139"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load data\n",
        "\n",
        "MyParser = HTML_folder_parser(folder_path, project_prefix)\n",
        "\n",
        "unique_files = MyParser.find_files_and_attachments()\n",
        "\n",
        "# CHECK: remove the sample from this\n",
        "#unique_files = unique_files.sample(5000)\n",
        "\n",
        "unique_entries = MyParser.read_and_parse_html(unique_files)\n",
        "\n",
        "# Filter those values that this is not processing yet:\n",
        "exclude_values = ['jpg', 'png', 'docx'] # CHECK!\n",
        "unique_entries[~unique_entries['extension'].isin(exclude_values)]\n",
        "\n",
        "pd.to_pickle(unique_entries, 'DATA/processed_html_data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63e0670a",
      "metadata": {
        "id": "63e0670a"
      },
      "outputs": [],
      "source": [
        "# Identify rows with removed data. Just for tracking (this info is not used).\n",
        "print(unique_entries['comment'].str.contains(\"[EMAIL REMOVED]\", regex=False).value_counts())\n",
        "print(unique_entries['comment'].str.contains(\"[NAME REMOVED]\", regex=False).value_counts())\n",
        "print(unique_entries['comment'].str.contains(\"[ADDRESS REMOVED]\", regex=False).value_counts())\n",
        "\n",
        "\n",
        "#CHECK! See attached file(s) ARE NOT REPEATED - UNLESS PROVEN OTHERWISE IN THEIR text\n",
        "#HOW DO I HANDLE TEXT INSIDE PDFS FOR LABELING?\n",
        "\n",
        "# Create a Series with the same index as unique_entries, containing the count of occurrences for each entry_code\n",
        "entry_code_counts = unique_entries['entry_code'].map(unique_entries['entry_code'].value_counts(dropna=False))\n",
        "\n",
        "# Now use this Series for boolean indexing\n",
        "duplicate_entry_codes = unique_entries[entry_code_counts > 1]\n",
        "\n",
        "# Now, duplicate_entry_codes should contain the rows from unique_entries where entry_code occurs more than once\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "752c30aa",
      "metadata": {
        "id": "752c30aa"
      },
      "outputs": [],
      "source": [
        "\n",
        "unique_entries = pd.read_pickle('DATA/processed_html_data')\n",
        "\n",
        "# Run deduplication comparing to find elements sharing common text. This approach uses inverse term frequency to give more weight to elements that appear less often. Then it generates vector representations of the text and compare their cosine similarity.\n",
        "\n",
        "from DeDuplication import RecordLinkage\n",
        "\n",
        "# Create an instance of the RecordLinkage class\n",
        "DeDuplicator = RecordLinkage(unique_entries, 'comment', n_gramsize=[8])\n",
        "\n",
        "# Perform de-duplication\n",
        "deduplicated = DeDuplicator.deduplication(ntop=100000, lower_bound=0.6, max_matches=None, n_grams=[8], full_words=True)\n",
        "# ntop parameters is in case i want a maximum number of entries for each to be matched, but in principle there is no limit to the number of matches here\n",
        "\n",
        "# Now we get the mapping of the new categories to group_ids\n",
        "new_group_ids = DeDuplicator.generate_group_ids(deduplicated)\n",
        "unique_entries = pd.merge(unique_entries, new_group_ids,\n",
        "                         how='outer',\n",
        "                         on='comment')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5beac454",
      "metadata": {
        "id": "5beac454"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Descriptive statistics on repeated text or mass campaigns\n",
        "unique_entries['repetitions_count'] = unique_entries.groupby('group_id')['group_id'].transform('count')\n",
        "unique_entries['is_campaign'] = unique_entries['repetitions_count']>5\n",
        "print(unique_entries['is_campaign'].value_counts())\n",
        "unique_entries['repetitions_count'].value_counts().hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "125c6d42",
      "metadata": {
        "id": "125c6d42"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Now we merge the unlabeled data in all entries with those already labeled:\n",
        "\n",
        "merged_labeled_data = pd.merge(left=unique_entries, right=labeled_data,\n",
        "                       left_on=['entry_code'],\n",
        "                       right_on=['entry_code_short'],\n",
        "                       indicator='label_merge',\n",
        "                       how='outer')\n",
        "\n",
        "merged_labeled_data = merged_labeled_data[merged_labeled_data['parsed_html'].isna()==False]\n",
        "# Keep unlabeled examples to test model below:\n",
        "unlabeled_data = merged_labeled_data[merged_labeled_data['label_merge']=='left_only']\n",
        "# Separate labeled data to train and test model:\n",
        "merged_labeled_data = merged_labeled_data[merged_labeled_data['label_merge']=='both']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98d7b317",
      "metadata": {
        "id": "98d7b317"
      },
      "outputs": [],
      "source": [
        "\n",
        "pd.to_pickle(unlabeled_data, 'DATA/unlabeled_data_pickle')\n",
        "\n",
        "model_data = merged_labeled_data[['entry_code_short', 'DESCRIPT', 'numeric_labels', 'comment']]\n",
        "model_data = merged_labeled_data[['comment', 'numeric_labels']]\n",
        "pd.to_pickle(model_data, 'DATA/model_data_pickle')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebdc380f",
      "metadata": {
        "id": "ebdc380f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "128978a1",
      "metadata": {
        "id": "128978a1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "f393de8f",
      "metadata": {
        "id": "f393de8f"
      },
      "source": [
        "# Starting from processed data:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f64c73d",
      "metadata": {
        "id": "4f64c73d"
      },
      "outputs": [],
      "source": [
        "\n",
        "# To load the dictionary back from the JSON file:\n",
        "with open('DATA/description_mapping.json', 'r') as file:\n",
        "    description_mapping = json.load(file)\n",
        "\n",
        "unlabeled_data = pd.read_pickle('DATA/unlabeled_data_pickle')\n",
        "\n",
        "model_data = pd.read_pickle('DATA/model_data_pickle')\n",
        "text_varname = 'comment'\n",
        "labels_varname = 'numeric_labels'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f3172de",
      "metadata": {
        "id": "6f3172de"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Instantiate the BERT Classifier\n",
        "MyClassifier = ClassifierBERT(model_data, text_varname, labels_varname, model='distilbert-base-uncased')\n",
        "\n",
        "\n",
        "new_data = MyClassifier.generate_augmented_data(train_df, len(train_df), shuffle=True)\n",
        "\n",
        "# Train only non-BERT layers first:\n",
        "MyClassifier.freeze_BERT_layers(unfreeze=False)\n",
        "MyClassifier.train_model(epochs=30, batch_size=16, train_size=0.7, test_size=0.15, val_size=0.15,\n",
        "                         initial_learning_rate=3e-5)\n",
        "\n",
        "# Train the model with all layers:\n",
        "MyClassifier.freeze_BERT_layers(unfreeze=True)\n",
        "MyClassifier.train_model(epochs=1, batch_size=16, train_size=0.7, test_size=0.15, val_size=0.15,\n",
        "                         initial_learning_rate=3e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5c7c56f",
      "metadata": {
        "id": "c5c7c56f"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Save trained model:\n",
        "MyClassifier.save_model('DATA/Trained_Classifier_SHORT')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb0bd687",
      "metadata": {
        "id": "bb0bd687"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load model:\n",
        "MyClassifier_loaded = ClassifierBERT.load_model('DATA/Trained_Classifier_SHORT')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01a73791",
      "metadata": {
        "id": "01a73791"
      },
      "outputs": [],
      "source": [
        "# Make predictions on unlabeled data\n",
        "unlabeled_data_unique = unlabeled_data[['comment','group_id']].drop_duplicates(subset='group_id', keep='first', inplace=False)\n",
        "unlabeled_text_varname = 'comment'\n",
        "unlabeled_data_unique = unlabeled_data_unique.sample(500)\n",
        "\n",
        "df_human_readable_labels = MyClassifier_loaded.predict_unlabeled_data(unlabeled_data_unique, unlabeled_text_varname, description_mapping)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98e7dca6",
      "metadata": {
        "id": "98e7dca6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "855b68de",
      "metadata": {
        "id": "855b68de"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}